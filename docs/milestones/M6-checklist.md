# Milestone 6 Checklist (LLM Tutor Reply via Ollama + Qwen3-8B)

## Goal

Verify assistant replies are generated by local `qwen3:8b` and respect learning settings.

## Setup

1. Start backend and web as in M5.
2. Ensure Ollama is running:
   - `ollama serve`
   - `ollama pull qwen3:8b` (first time only)
3. Confirm debug row shows `WS: connected`.

## Pass/Fail Tests

1. Basic LLM response
- Type: `こんにちは。今日は日本語を練習したいです。`
- Click `Send`.
- Assistant reply should be generated (not `[Mock ...]` text).
- Debug should show `LLM: done`.

2. Model and latency visible
- Open debug panel.
- Confirm `LLM model` is populated (expected: `qwen3:8b` or equivalent model tag).
- Confirm `LLM latency` is not `-` after a successful reply.

3. Reply Language setting
- Set `Reply Language` to `Japanese only`, send English input.
- Reply should be Japanese only.
- Set `Reply Language` to `English first`, send Japanese input.
- Reply should be English first with optional Japanese examples.

4. Correction Intensity setting
- Send a sentence with obvious mistakes.
- With `Light`, correction should be brief.
- With `Heavy`, correction should be more explicit and detailed.

5. Response Length setting
- Ask same question under `Very short` and `Detailed`.
- `Detailed` response should be clearly longer than `Very short`.

6. Voice flow still works
- Record voice, wait for STT to fill input, then send.
- User voice bubble playback (`[▶]`) should still work and assistant should reply via LLM.

## Exit Criteria

Milestone 6 is done only if all six tests pass.
